{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c761ce98-ee27-4b21-afc9-f32b7e8a8c8c",
   "metadata": {},
   "source": [
    "# COGS18 -- Final Project:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb5474d-3f7d-4de3-8128-dd5c1be6af29",
   "metadata": {},
   "source": [
    "# Automated Text Analysis of Newspaper Headlines:\n",
    "\n",
    "Initial goals:\n",
    "1. Webscrape the biggest Norwegian newspapers, preferably at a set time each day, over a limited time period.\n",
    "2. Look at frequency of word use in headlines and visualize this\n",
    "3. Perform a sentiment analysis, to see wether there are noticable differences in the language used in the different newspapers.\n",
    "\n",
    "\n",
    "I was able to webscrape four Norwegian newspapers (that allow web scraping) usually at 5 pm between 1-9 December 2024. I encountered issues with my automated scraping because of lack of internet connection, which made me add a function to check internet connectivity and to try again at a later time.\n",
    "\n",
    "Later, I had trouble with the Norwegian characthers \"Æ Ø Å\", which I also had to be concious about during my data collection. I also changed my script to save the data in a CSV for each day, and rather later merge them, using code in script \"part 2\". This is because I had one instance where the data was collected in a CSV during testing, but because of lack of internet connection my function simply just overwrote the CSV, essentially deleting the existing data. This lead to me adding a retry-function. If I were to further develop this I would make sure the testing and the actual data collection is storing data in different CSVs.\n",
    "\n",
    "I did spend a lot of time trying to train and optimize BERT (NB-BERT-base is a general BERT-base model built on the large digital collection at the National Library of Norway) on the Norwegian dataset NoReC_fine (based largely on the original data described in the paper A Fine-Grained Sentiment Dataset for Norwegian) in order to perform a sentiment analysis, since there are a lack of sentiment tools developed for the Norwegian language. Unfortunately, this became too difficult and time consuming for the assignment, later I decided to rather explore some ways of measuring correlation between the newspapers word usage. However, this also became too complicated and I decided to abandon my initial goal to rather immerse myself in goal 1. and 2.\n",
    "\n",
    "I was however able to calculate and plot the frequency of the words used in the newspapers using both barplots and wordclouds. The visualization was essentially dependent on a lot of considerate steps in preprocessing, like adding Norwegian stop words so the plots made sense in terms of content and sentiment. I also decided to add this as a separate script after a short period of hardcoding the stop words in each of the scripts.\n",
    "\n",
    "Overall, this was a very rich project providing me with the experience of collecting data from the web, preprocessing it, and thereafter visualizing it, in order to make sense of the data -- from a human perspective. Given the step-by-step approach builiding three individual main scripts (and the stop words-script), this method provides a highly scalable and customizeable approach to collecting, processing, and making sense of data fetched from the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05004184-2865-47be-b264-de30a20fa6f7",
   "metadata": {},
   "source": [
    "# Workflow and Scripts\n",
    "\n",
    "The project consists of three main parts, and one smaller script (plus test_functions-script):\n",
    "\n",
    "# Part 1: Data Collection:\n",
    "\n",
    "    Script: part1_news_scraper.py\n",
    "    \n",
    "    Description:\n",
    "    - Scraper function to collect headlines from multiple sources, and store headlines with metadata (date, time, source) in individual CSV files.\n",
    "\n",
    "# Part 2: Data Cleaning and Merging:\n",
    "\n",
    "    Script: part2_clean_and_merge.py\n",
    "    \n",
    "    Description:\n",
    "    - Clean and preprocess the scraped data to remove irrelevant characters, normalize text, and remove stopwords, and merge all CSVs.\n",
    "    \n",
    "\n",
    "# Part 3: Word Frequency Analysis and Visualization:\n",
    "\n",
    "    Script: part3_analyze_and_visualize.py\n",
    "    \n",
    "    Description:\n",
    "    - Generates word clouds and frequency bar charts to visualize the frequency of words for all newspapers combined and each newspaper individually. (Top 20 words overall, and top 10 words for each newspaper).\n",
    "\n",
    "# Additional Script: Stop Words and Remove Stop Words-Function:\n",
    "\n",
    "    Script: (stopwords.py)\n",
    "    Description:\n",
    "    - Removed Norwegian stop words and added a function to easier remove stopwords for data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf9bfca-b6ed-45f7-a97d-9022f677f30f",
   "metadata": {},
   "source": [
    "# Scripts:\n",
    "\n",
    "Location: cd ~/Desktop/Github/final_project_COGS18/assignment_folder\n",
    "\n",
    "1. python3 part1_news_scraper.py\n",
    "2. python3 part2_clean_and_merge.py\n",
    "3. python3 part3_analyze_and_visualize.py\n",
    "\n",
    "4. python3 stopwords.py\n",
    "\n",
    "5. python3 test_functions.py\n",
    "\n",
    "   \n",
    "# Python Libraries:\n",
    "\n",
    "- pandas: For data handling and cleaning.\n",
    "- matplotlib, seaborn, WordCloud: For visualization.\n",
    "- Counter: For word frequency analysis.\n",
    "- scipy.cluster.hierarchy, sklearn.metrics.pairwise: For similarity metrics and clustering.\n",
    "- bs4/BeatifulSoup: For web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c14156-2e78-438b-8016-7a31bfb3263c",
   "metadata": {},
   "source": [
    "# Part 1:\n",
    "\n",
    "My newspaper web scraper script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf32fe3-5e33-46ed-affd-0eb5f2ce6931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduling scraper to run at 8 AM San Diego time (5 PM Norway time).\n",
      "Test run scheduled at: 20:22\n",
      "Scheduler is running...\n"
     ]
    }
   ],
   "source": [
    "# part1_news_scraper.py\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import schedule\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import socket\n",
    "\n",
    "# First, define websites and the CSS selectors\n",
    "websites = {\n",
    "    \"NRK\": {\"url\": \"https://www.nrk.no/\", \"selector\": \"h2.kur-room__title\"},\n",
    "    \"VG\": {\"url\": \"https://www.vg.no/\", \"selector\": \".headline\"},\n",
    "    \"Aftenposten\": {\"url\": \"https://www.aftenposten.no/\", \"selector\": \"h2\"},\n",
    "    \"Se og Hør\": {\"url\": \"https://www.seher.no/\", \"selector\": \"h2.headline\"}\n",
    "}\n",
    "\n",
    "# function that checks internet connectivity\n",
    "def is_connected():\n",
    "    try:\n",
    "        socket.create_connection((\"8.8.8.8\", 53), timeout=5)\n",
    "        return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "# function to clean up text by normalizing space\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove excess whitespace, line breaks, and normalize spaces.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# function to scrape a single website\n",
    "def scrape_website(url, headline_selector):\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response.encoding = \"utf-8\"  # to make sure it understands the Norwegian characters\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        headlines = soup.select(headline_selector)\n",
    "        \n",
    "        # fetch and clean up headlines\n",
    "        cleaned_headlines = []\n",
    "        for headline in headlines:\n",
    "            # join all parts of the text and ensuring spacing\n",
    "            full_text = \" \".join(headline.stripped_strings)  # Handles nested tags\n",
    "            cleaned_headlines.append(clean_text(full_text))\n",
    "        \n",
    "        return cleaned_headlines\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# function to scrape and save data\n",
    "def scrape_daily(retry_count=0):\n",
    "    max_retries = 3\n",
    "\n",
    "    # checks internet connectivity\n",
    "    if not is_connected():\n",
    "        print(\"No internet connection. Retrying in 1 hour...\")\n",
    "        if retry_count < max_retries:\n",
    "            schedule.every(1).hour.do(lambda: scrape_daily(retry_count + 1))\n",
    "        else:\n",
    "            print(\"Max retries reached. Scraping aborted.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(\"Scraping started...\")\n",
    "        all_headlines = []\n",
    "\n",
    "        # scrapes each of the Norwegian newspaper websites\n",
    "        for site, info in websites.items():\n",
    "            print(f\"Scraping {site}...\")\n",
    "            headlines = scrape_website(info[\"url\"], info[\"selector\"])\n",
    "            print(f\"Collected {len(headlines)} headlines for {site}\")\n",
    "            for headline in headlines:\n",
    "                all_headlines.append({\n",
    "                    \"Source\": site,\n",
    "                    \"Headline\": headline,\n",
    "                    \"Date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "                    \"Time\": datetime.now().strftime(\"%H:%M:%S\")\n",
    "                })\n",
    "            time.sleep(1)  # added a delay between requests\n",
    "\n",
    "        # saves results to a CSV file for each day\n",
    "        if all_headlines:\n",
    "            date_today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            file_name = f\"news_headlines_{date_today}.csv\"\n",
    "            df = pd.DataFrame(all_headlines)\n",
    "            df.to_csv(file_name, index=False, encoding=\"utf-8\")\n",
    "            print(f\"Scraping complete! Data saved to '{file_name}'.\")\n",
    "        else:\n",
    "            print(\"No headlines collected. No file saved.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Scraping failed: {e}\")\n",
    "        if retry_count < max_retries:\n",
    "            print(f\"Retrying in 1 hour... Attempt {retry_count + 1}/{max_retries}\")\n",
    "            schedule.every(1).hour.do(lambda: scrape_daily(retry_count + 1))\n",
    "        else:\n",
    "            print(\"Max retries reached. Scraping aborted.\")\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "\t# schedules the scraper to run daily at 8 AM San Diego time (== 5 PM Norway time)\n",
    "\tprint(\"Scheduling scraper to run at 8 AM San Diego time (5 PM Norway time).\")\n",
    "\tschedule.every().day.at(\"08:00\").do(scrape_daily)\n",
    "\n",
    "\t# test scheduler in one minute for debugging\n",
    "\ttest_time = (datetime.now() + timedelta(minutes=1)).strftime(\"%H:%M\")\n",
    "\tprint(f\"Test run scheduled at: {test_time}\")\n",
    "\tschedule.every().day.at(test_time).do(scrape_daily)\n",
    "\n",
    "\t# run the scheduler\n",
    "\tprint(\"Scheduler is running...\")\n",
    "\twhile True:\n",
    "    \t\tschedule.run_pending()\n",
    "    \t\ttime.sleep(1)  # Waits one second before checking again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dff4b6-a20a-4ab1-8fa7-8e1c7552f132",
   "metadata": {},
   "source": [
    "# Part 2.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59dfd6-fc94-4056-9cd6-5ab93e6c92a1",
   "metadata": {},
   "source": [
    "My stopwords script + remove stopwords_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b699b61-1de8-45f3-9309-ea07beb9ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "norwegian_stopwords = [\n",
    "    \"jeg\", \"du\", \"han\", \"hun\", \"det\", \"de\", \"er\", \"en\", \"og\", \"på\", \"som\", \"med\", \"til\", \"fra\",\n",
    "    \"for\", \"av\", \"var\", \"vi\", \"kan\", \"ha\", \"nå\", \"har\", \"om\", \"et\", \"seg\", \"mot\", \"ut\", \"får\",\n",
    "    \"ble\", \"ikke\", \"bare\", \"alle\", \"må\", \"den\", \"så\", \"sin\", \"man\", \"og\", \"i\", \"kunne\", \"hva\",\n",
    "    \"hvordan\", \"der\", \"når\", \"alt\", \"år\", \"vil\", \"igjen\", \"skal\", \"noen\", \"deg\", \"meg\", \"dette\",\n",
    "    \"andre\", \"bli\", \"sa\", \"ved\", \"etter\", \"hvor\", \"selv\", \"noe\", \"disse\", \"opp\", \"men\", \"oss\",\n",
    "    \"over\", \"nå\", \"vårt\", \"nye\", \"helt\", \"få\", \"gjør\", \"blir\", \"hver\", \"ut\", \"inn\", \"da\", \"før\",\n",
    "    \"veldig\", \"min\", \"ny\", \"litt\", \"vår\", \"si\", \"kommer\", \"rundt\", \"hvem\", \"hvorfor\", \"være\",\n",
    "    \"aldri\", \"fikk\", \"gå\", \"gjøre\", \"dere\", \"flere\", \"mest\", \"bare\", \"først\", \"eller\", \"gikk\",\n",
    "    \"ned\", \"dette\", \"slik\", \"jo\", \"skulle\", \"vil\", \"nok\", \"mens\", \"egentlig\", \"sånn\", \"enn\",\n",
    "    \"alle\", \"andre\", \"at\", \"av\", \"bare\", \"begge\", \"ble\", \"blei\", \"bli\", \"blir\",\n",
    "    \"blitt\", \"bort\", \"bra\", \"bruke\", \"både\", \"båe\", \"da\", \"de\", \"deg\", \"dei\", \"deim\", \"deira\",\n",
    "    \"deires\", \"dem\", \"den\", \"denne\", \"der\", \"dere\", \"deres\", \"det\", \"dette\", \"di\", \"din\", \"disse\",\n",
    "    \"ditt\", \"du\", \"dykk\", \"dykkar\", \"då\", \"eg\", \"ein\", \"eit\", \"eitt\", \"eller\", \"elles\", \"en\",\n",
    "    \"ene\", \"eneste\", \"enhver\", \"enn\", \"er\", \"et\", \"ett\", \"etter\", \"folk\", \"for\", \"fordi\",\n",
    "    \"forsûke\", \"fra\", \"få\", \"før\", \"fûr\", \"fûrst\", \"gjorde\", \"gjûre\", \"god\", \"gå\", \"ha\", \"hadde\",\n",
    "    \"han\", \"hans\", \"har\", \"hennar\", \"henne\", \"hennes\", \"her\", \"hjå\", \"ho\", \"hoe\", \"honom\", \"hoss\",\n",
    "    \"hossen\", \"hun\", \"hva\", \"hvem\", \"hver\", \"hvilke\", \"hvilken\", \"hvis\", \"hvor\", \"hvordan\",\n",
    "    \"hvorfor\", \"i\", \"ikke\", \"ikkje\", \"ingen\", \"ingi\", \"inkje\", \"inn\", \"innen\", \"inni\", \"ja\",\n",
    "    \"jeg\", \"kan\", \"kom\", \"korleis\", \"korso\", \"kun\", \"kunne\", \"kva\", \"kvar\", \"kvarhelst\", \"kven\",\n",
    "    \"kvi\", \"kvifor\", \"lage\", \"lang\", \"lik\", \"like\", \"makt\", \"man\", \"mange\", \"me\", \"med\", \"medan\",\n",
    "    \"meg\", \"meget\", \"mellom\", \"men\", \"mens\", \"mer\", \"mest\", \"mi\", \"min\", \"mine\", \"mitt\", \"mot\",\n",
    "    \"mye\", \"mykje\", \"må\", \"måte\", \"navn\", \"ned\", \"nei\", \"no\", \"noe\", \"noen\", \"noka\", \"noko\",\n",
    "    \"nokon\", \"nokor\", \"nokre\", \"ny\", \"nå\", \"når\", \"og\", \"også\", \"om\", \"opp\", \"oss\", \"over\",\n",
    "    \"part\", \"punkt\", \"på\", \"rett\", \"riktig\", \"samme\", \"sant\", \"seg\", \"selv\", \"si\", \"sia\",\n",
    "    \"sidan\", \"siden\", \"sin\", \"sine\", \"sist\", \"sitt\", \"sjøl\", \"skal\", \"skulle\", \"slik\", \"slutt\",\n",
    "    \"so\", \"som\", \"somme\", \"somt\", \"start\", \"stille\", \"så\", \"sånn\", \"tid\", \"til\", \"tilbake\",\n",
    "    \"tilstand\", \"um\", \"under\", \"upp\", \"ut\", \"uten\", \"var\", \"vart\", \"varte\", \"ved\", \"verdi\",\n",
    "    \"vere\", \"verte\", \"vi\", \"vil\", \"ville\", \"vite\", \"vore\", \"vors\", \"vort\", \"vår\", \"være\",\n",
    "    \"vært\", \"vöre\", \"vört\", \"å\", \"gang\", \"første\", \"fortsatt\", \"se\", \"stor\", \"går\", \"dagens\",\n",
    " \"les\", \"n\", \"frå\", \"nær\", \"ser\", \"en\", \"to\", \"tre\", \"fire\", \"fem\", \"seks\", \"syv\", \"åtte\",\n",
    "\"ni\", \"elleve\", \"tolv\", \"egen\", \"nrk\", \"blant\", \"sett\", \"hatt\", \"ham\", \"han\", \"ifølge\", \"følge\", \"hele\",\n",
    "\"fått\", \"radio\", \"radioprogrammer\", \"podkast\", \"podcast\", \"podkaster\", \"podcaster\", \"våre\", \"Hallo\", \"hallo\"\n",
    "]\n",
    "\n",
    "# Added a function to remove stop words in data cleaning\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    \"\"\"\n",
    "    Remove stopwords from the text.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): input text for processing.\n",
    "        stopwords (list): list of stopwords to remove.\n",
    "        \n",
    "    Returns:\n",
    "        str: Text without stopwords.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return \" \".join(word for word in words if word.lower() not in stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b659f4-b8d2-41c3-9eef-e17999b42d89",
   "metadata": {},
   "source": [
    "# Part 2.2:\n",
    "\n",
    "My script for cleaning and merging the data in the collected CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c1cebc-f459-4e45-b26e-cc9c6af78005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part2_clean_and_merge.py\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from stopwords import norwegian_stopwords, remove_stopwords  # Import the stopword list for Norwegian words and the remove_stopwords function\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text strings by removing non-alphanumeric characters, \n",
    "    excessive whitespace, and normalizing the text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # remove special characters, numbers, and excessive whitespace\n",
    "    text = re.sub(r\"[^a-zA-ZæøåÆØÅ\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    \"\"\"\n",
    "    Removes stopwords from the text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return \" \".join(word for word in words if word.lower() not in stopwords)\n",
    "\n",
    "def clean_and_merge_csvs(input_folder, output_file):\n",
    "    \"\"\"\n",
    "    Merge and clean multiple CSV files into one consolidated file.\n",
    "    \"\"\"\n",
    "    all_data = []  # list to store data from all CSVs\n",
    "    \n",
    "    # Iterates over all CSV files in the folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            try:\n",
    "                # Reads CSV file\n",
    "                df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "                \n",
    "                # Drops rows that have a missing values\n",
    "                df.dropna(subset=[\"Headline\", \"Source\"], inplace=True)\n",
    "                \n",
    "                # Removes duplicate headlines\n",
    "                df.drop_duplicates(subset=[\"Headline\"], inplace=True)\n",
    "                \n",
    "                # Cleans the 'Headline' column\n",
    "                df[\"Headline\"] = df[\"Headline\"].apply(clean_text)\n",
    "                \n",
    "                # Removes stopwords from the 'Headline' column\n",
    "                df[\"Headline\"] = df[\"Headline\"].apply(lambda x: remove_stopwords(x, norwegian_stopwords))\n",
    "                \n",
    "                # Removes rows with empty headlines after cleaning\n",
    "                df = df[df[\"Headline\"].str.strip().astype(bool)]\n",
    "                \n",
    "                all_data.append(df)  # Appends the cleaned data to the list\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    # Combines all cleaned data into a single DataFrame\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"Total rows after merging: {len(combined_df)}\")\n",
    "        \n",
    "        # Save the DataFrame to a new CSV file\n",
    "        combined_df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Cleaned and merged data saved to '{output_file}'.\")\n",
    "    else:\n",
    "        print(\"No valid data found to merge.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"headlines_csv\"\n",
    "    output_file = \"merged_cleaned_headlines.csv\"\n",
    "    clean_and_merge_csvs(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67044cc6-fecb-48be-b061-5992cc17596c",
   "metadata": {},
   "source": [
    "# Part 3:\n",
    "\n",
    "My script for counting and displaying the most frequently used words, thereafter visualizing the most frequent words in our collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1573c2-693a-4016-b63d-6e23f0518148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part3_analyze_and_visualize.py\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from stopwords import norwegian_stopwords, remove_stopwords  # Import the stopword list for Norwegian words and the remove_stopwords function\n",
    "\n",
    "# Useful Functions\n",
    "\n",
    "def generate_wordcloud_text(df, column, stopwords):\n",
    "    \"\"\"\n",
    "    Generates cleaned text for a word cloud.\n",
    "    \"\"\"\n",
    "    return \" \".join(remove_stopwords(text, stopwords) for text in df[column])\n",
    "\n",
    "def calculate_word_frequencies(df, column, stopwords, top_n=20):\n",
    "    \"\"\"\n",
    "    Calculates word frequencies for a specific column.\n",
    "    \"\"\"\n",
    "    all_text = generate_wordcloud_text(df, column, stopwords)\n",
    "    word_counts = Counter(all_text.split())\n",
    "    return word_counts.most_common(top_n)\n",
    "\n",
    "def plot_combined_wordclouds(df, stopwords):\n",
    "    \"\"\"\n",
    "    Combined word cloud visualization for all the newspapers.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 6))\n",
    "\n",
    "    for i, (source, group) in enumerate(df.groupby(\"Source\")):\n",
    "        # Generates the word cloud text\n",
    "        source_text = generate_wordcloud_text(group, \"Headline\", stopwords)\n",
    "        wordcloud = WordCloud(width=400, height=400, background_color=\"white\").generate(source_text)\n",
    "        \n",
    "        # Plot the word cloud\n",
    "        axs[i].imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        axs[i].axis(\"off\")\n",
    "        axs[i].set_title(f\"Word Cloud - {source}\", fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_combined_frequencies(df, stopwords):\n",
    "    \"\"\"\n",
    "    Combined frequency bar chart visualization for all the newspapers.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 6))\n",
    "\n",
    "    for i, (source, group) in enumerate(df.groupby(\"Source\")):\n",
    "        # Calculate the word frequencies\n",
    "        source_word_counts = calculate_word_frequencies(group, \"Headline\", stopwords, top_n=10)\n",
    "        words, counts = zip(*source_word_counts)\n",
    "\n",
    "        # Plot the frequencies\n",
    "        axs[i].barh(words, counts, color=\"skyblue\")\n",
    "        axs[i].set_title(f\"Word Frequencies - {source}\", fontsize=14)\n",
    "        axs[i].set_xlabel(\"Frequency\")\n",
    "        axs[i].invert_yaxis()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the cleaned and merged CSV\n",
    "    input_file = \"merged_cleaned_headlines.csv\"\n",
    "    df = pd.read_csv(input_file, encoding=\"utf-8\")\n",
    "    print(f\"Loaded {len(df)} headlines from '{input_file}'.\")\n",
    "\n",
    "    # Combined Word Clouds\n",
    "    print(\"\\n Combined Word Clouds for All Newspapers \")\n",
    "    plot_combined_wordclouds(df, norwegian_stopwords)\n",
    "\n",
    "    # Combined Frequencies\n",
    "    print(\"\\n Combined Frequencies for All Newspapers \")\n",
    "    plot_combined_frequencies(df, norwegian_stopwords)\n",
    "\n",
    "    # Overall Analysis\n",
    "    print(\"\\n Overall Analysis \")\n",
    "    overall_text = generate_wordcloud_text(df, \"Headline\", norwegian_stopwords)\n",
    "    overall_word_counts = calculate_word_frequencies(df, \"Headline\", norwegian_stopwords, top_n=20)\n",
    "\n",
    "    # Plot Word Cloud (all)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(overall_text)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud - All Newspapers Combined\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Frequencies (all)\n",
    "    words, counts = zip(*overall_word_counts)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(words, counts, color=\"skyblue\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.title(\"Word Frequencies - All Newspapers Combined\", fontsize=16)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4fc32-99eb-4bbf-80a6-deb13cd30f4a",
   "metadata": {},
   "source": [
    "# Script to test functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c316ba7-2950-4e1a-8101-446286967e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 test_functions.py\n",
    "\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "from part1_news_scraper import scrape_website\n",
    "from part2_clean_and_merge import clean_text\n",
    "from part3_analyze_and_visualize import generate_wordcloud_text\n",
    "from stopwords import norwegian_stopwords\n",
    "\n",
    "def test_scrape_function():\n",
    "    print(\"Testing scrape_website...\")\n",
    "    websites = {\n",
    "        \"NRK\": {\"url\": \"https://www.nrk.no/\", \"selector\": \"h2.kur-room__title\"}\n",
    "    }\n",
    "    try:\n",
    "        # Run the scraper\n",
    "        result = scrape_website(websites[\"NRK\"][\"url\"], websites[\"NRK\"][\"selector\"])\n",
    "        assert isinstance(result, list), \"scrape_website did not return a list\"\n",
    "        print(f\"Scrape function works as intended! Collected {len(result)} headlines.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Test failed: {e}\")\n",
    "\n",
    "def test_clean_text_function():\n",
    "    print(\"Testing clean_text...\")\n",
    "    try:\n",
    "        dirty_text = \"Hallo, Verden! 123\"\n",
    "        cleaned = clean_text(dirty_text)\n",
    "        assert cleaned == \"Hallo Verden\", f\"clean_text failed: {cleaned}\"\n",
    "        print(\"Clean text function works as intended!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Test failed: {e}\")\n",
    "\n",
    "def test_generate_wordcloud_text_function():\n",
    "    print(\"Testing generate_wordcloud_text...\")\n",
    "    try:\n",
    "        data = pd.DataFrame([{\"Headline\": \"Hallo Verden\"}, {\"Headline\": \"Norge er et vakkert land\"}])\n",
    "        wordcloud_text = generate_wordcloud_text(data, \"Headline\", norwegian_stopwords)\n",
    "        assert isinstance(wordcloud_text, str), \"generate_wordcloud_text didnt return a string\"\n",
    "        assert \"Hallo\" not in wordcloud_text, \"Stopwords were not removed correctly\"\n",
    "        assert \"Verden\" in wordcloud_text, \"Wordcloud text output is incorrect\"\n",
    "        print(\"Generate wordcloud text function works as intended!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Test failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_scrape_function()\n",
    "    test_clean_text_function()\n",
    "    test_generate_wordcloud_text_function()\n",
    "    print(\"All tests are completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabdbc9-303b-4d46-9a62-00a6a8f7c9b5",
   "metadata": {},
   "source": [
    "# Extra Credit (up to 4%)\n",
    "\n",
    "For extra credit, if you go above and beyond on the minimal project requirements and challenge yourself to approach a project that is more complex than the basic requirements, requires you to learn something beyond what was taught in the course, or uses code concepts not taught in class, explain this at the end of your Jupyter notebook. Here, you should explain why your approach was particularly difficult/challenging for you and how your work goes beyond the minimal project requirements.\n",
    "\n",
    "- I believe this project was both very meaningful in terms of my learning data collection, processing, and visualization, and I believe that the extent of the project challenged me beyond what would have been necessary if my only goal was to pass the course, rather I spent a large amount of time working on different approaches (like sentiment analysis and correlation matrixes, even though they did not directly yield results in the ways I intended them to do). As well as creating four different scripts, tackeling challenges like Norwegian letters and more.\n",
    "\n",
    "  \n",
    "# GitHub Extra Credit (1%)\n",
    "\n",
    "For extra credit, if you put your project in a public repo on GitHub, you will earn 1% extra credit on the project. (Note: This should be the unzipped version of your project, so that others on GitHub can see your code.)\n",
    "\n",
    "- I'll publish it on: https://github.com/fishtangerine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ef3fe-82c8-4c4f-a46b-78fa6fbcf064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
